{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f17658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\")\n",
    "admin_key= os.getenv(\"ADMIN_KEY\")\n",
    "index_name = os.getenv(\"INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchableField, SearchFieldDataType\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Set your Azure Search details\n",
    "endpoint = search_endpoint\n",
    "admin_key = admin_key\n",
    "credential = AzureKeyCredential(admin_key)\n",
    "\n",
    "# Index names\n",
    "indexes = [\"finance\", \"hr\", \"engineering\"]\n",
    "csv_files = {\n",
    "    \"finance\": \"finance.csv\",\n",
    "    \"hr\": \"hr.csv\",\n",
    "    \"engineering\": \"engineering.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6396a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name):\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True, sortable=True, facetable=False),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String, filterable=False, sortable=False, facetable=False),\n",
    "        SimpleField(name=\"timestamp\", type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True, facetable=False),\n",
    "        SimpleField(name=\"score\", type=SearchFieldDataType.Double, filterable=True, sortable=True, facetable=False)\n",
    "    ]\n",
    "    index = SearchIndex(name=index_name, fields=fields)\n",
    "    index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "    # Delete if exists\n",
    "    try:\n",
    "        index_client.delete_index(index_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    index_client.create_index(index)\n",
    "    print(f\"Index '{index_name}' created.\")\n",
    "\n",
    "for idx in indexes:\n",
    "    create_index(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_documents(index_name, csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Azure Search doesn't like '@' in field names, so rename\n",
    "    df = df.rename(columns={\"@search.score\": \"score\"})\n",
    "    # Convert timestamp to ISO format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    docs = df.to_dict(orient=\"records\")\n",
    "    search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "    result = search_client.upload_documents(documents=docs)\n",
    "    print(f\"Uploaded {len(docs)} docs to '{index_name}':\", result[0].status_code if result else \"OK\")\n",
    "\n",
    "for idx, file in csv_files.items():\n",
    "    upload_documents(idx, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "def search_index(index_name, search_text, top=10, filter_expr=None, orderby=None):\n",
    "    search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "    results = search_client.search(\n",
    "        search_text,\n",
    "        filter=filter_expr,\n",
    "        top=top,\n",
    "        order_by=orderby if orderby else None,\n",
    "        include_total_count=True\n",
    "    )\n",
    "    docs = []\n",
    "    for r in results:\n",
    "        doc = r.copy()\n",
    "        doc['__index'] = index_name\n",
    "        doc['score'] = r['score'] if 'score' in r else r['@search.score'] if '@search.score' in r else r['score']\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "# Query all indexes\n",
    "def query_all_indexes(search_text, top=10, filter_expr=None, orderby=\"timestamp desc\"):\n",
    "    all_results = []\n",
    "    for idx in indexes:\n",
    "        docs = search_index(idx, search_text, top=top, filter_expr=filter_expr, orderby=orderby)\n",
    "        all_results.extend(docs)\n",
    "    return all_results\n",
    "\n",
    "# Normalize scores\n",
    "def normalize_scores(results, score_field=\"score\"):\n",
    "    scores = [doc[score_field] for doc in results]\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    for doc in results:\n",
    "        doc[\"norm_score\"] = (doc[score_field] - min_s) / (max_s - min_s) if max_s > min_s else 1.0\n",
    "    return results\n",
    "\n",
    "# RRF merge with tie-breaking\n",
    "def rrf_merge(results_by_index: Dict[str, List[Dict]], k=60):\n",
    "    \"\"\"\n",
    "    Merge results using Reciprocal Rank Fusion.\n",
    "    \n",
    "    When RRF scores are identical, uses timestamp as tie-breaker (newest first).\n",
    "    This ensures consistent, predictable ordering across pages.\n",
    "    \"\"\"\n",
    "    doc_ranks = {}\n",
    "    for idx, docs in results_by_index.items():\n",
    "        docs_sorted = sorted(docs, key=lambda x: x[\"norm_score\"], reverse=True)\n",
    "        for rank, doc in enumerate(docs_sorted):\n",
    "            doc_id = doc[\"id\"]\n",
    "            if doc_id not in doc_ranks:\n",
    "                doc_ranks[doc_id] = []\n",
    "            doc_ranks[doc_id].append(rank + 1)\n",
    "    \n",
    "    merged = []\n",
    "    for doc_id, ranks in doc_ranks.items():\n",
    "        rrf_score = sum(1 / (r + k) for r in ranks)\n",
    "        doc = next(d for d in all_results if d[\"id\"] == doc_id)\n",
    "        merged.append({**doc, \"rrf_score\": rrf_score})\n",
    "    \n",
    "    # Sort by RRF score (primary), then timestamp (secondary tie-breaker)\n",
    "    # This ensures consistent ordering when RRF scores are equal\n",
    "    merged.sort(key=lambda x: (-x[\"rrf_score\"], x[\"timestamp\"]), reverse=True)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Paging by RRF score (recommended for relevance-based results)\n",
    "def get_page_by_rrf(results, page_num=1, page_size=10):\n",
    "    \"\"\"\n",
    "    Paginate results by RRF score (maintains relevance ranking).\n",
    "    Use this when you want to show results in order of relevance.\n",
    "    \"\"\"\n",
    "    start = (page_num - 1) * page_size\n",
    "    end = start + page_size\n",
    "    page = results[start:end]\n",
    "    \n",
    "    total_pages = (len(results) + page_size - 1) // page_size\n",
    "    has_next = page_num < total_pages\n",
    "    \n",
    "    return {\n",
    "        \"results\": page,\n",
    "        \"page\": page_num,\n",
    "        \"page_size\": page_size,\n",
    "        \"total_results\": len(results),\n",
    "        \"total_pages\": total_pages,\n",
    "        \"has_next\": has_next,\n",
    "        \"has_previous\": page_num > 1\n",
    "    }\n",
    "\n",
    "# Paging by timestamp (cursor-based - for time-sorted results)\n",
    "def get_page_by_timestamp(df, page_size=2, last_timestamp=None):\n",
    "    \"\"\"\n",
    "    Paginate results by timestamp (cursor-based).\n",
    "    Use this when you want to show results in chronological order.\n",
    "    Note: This ignores RRF scores and sorts purely by time.\n",
    "    \"\"\"\n",
    "    df = sorted(df, key=lambda x: x['timestamp'], reverse=True)\n",
    "    if last_timestamp:\n",
    "        df = [d for d in df if d['timestamp'] < last_timestamp]\n",
    "    page = df[:page_size]\n",
    "    next_cursor = page[-1]['timestamp'] if page else None\n",
    "    return page, next_cursor\n",
    "\n",
    "# Demo search with RRF-based pagination\n",
    "search_text = \"cloud security\"\n",
    "all_results = query_all_indexes(search_text, top=10)\n",
    "all_results = normalize_scores(all_results)\n",
    "# Group by index for RRF\n",
    "results_by_index = {idx: [doc for doc in all_results if doc[\"__index\"] == idx] for idx in indexes}\n",
    "merged_results = rrf_merge(results_by_index)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PAGINATION BY RRF SCORE (Maintains Relevance Ranking)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First page - shows highest RRF scores\n",
    "page_result = get_page_by_rrf(merged_results, page_num=1, page_size=2)\n",
    "print(f\"\\n📄 Page {page_result['page']} of {page_result['total_pages']}:\")\n",
    "for i, doc in enumerate(page_result['results'], 1):\n",
    "    print(f\"   {i}. [{doc['__index']:12}] {doc['title'][:40]:40} | RRF: {doc['rrf_score']:.4f} | {doc['timestamp']}\")\n",
    "\n",
    "# Second page - shows next highest RRF scores\n",
    "if page_result['has_next']:\n",
    "    page_result2 = get_page_by_rrf(merged_results, page_num=2, page_size=2)\n",
    "    print(f\"\\n📄 Page {page_result2['page']} of {page_result2['total_pages']}:\")\n",
    "    for i, doc in enumerate(page_result2['results'], 1):\n",
    "        print(f\"   {i}. [{doc['__index']:12}] {doc['title'][:40]:40} | RRF: {doc['rrf_score']:.4f} | {doc['timestamp']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PAGINATION BY TIMESTAMP (Chronological Order)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare with timestamp-based pagination\n",
    "page1, cursor1 = get_page_by_timestamp(merged_results, page_size=2)\n",
    "print(f\"\\n📄 Page 1 (newest first):\")\n",
    "for i, doc in enumerate(page1, 1):\n",
    "    print(f\"   {i}. [{doc['__index']:12}] {doc['title'][:40]:40} | RRF: {doc['rrf_score']:.4f} | {doc['timestamp']}\")\n",
    "print(f\"Next cursor: {cursor1}\")\n",
    "\n",
    "if cursor1:\n",
    "    page2, cursor2 = get_page_by_timestamp(merged_results, page_size=2, last_timestamp=cursor1)\n",
    "    print(f\"\\n📄 Page 2:\")\n",
    "    for i, doc in enumerate(page2, 1):\n",
    "        print(f\"   {i}. [{doc['__index']:12}] {doc['title'][:40]:40} | RRF: {doc['rrf_score']:.4f} | {doc['timestamp']}\")\n",
    "    print(f\"Next cursor: {cursor2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65defce9",
   "metadata": {},
   "source": [
    "# Optimized Multi-Index Search with Azure Redis Cache\n",
    "\n",
    "This implementation adds:\n",
    "1. **Azure Cache for Redis** - Managed Redis service from Azure\n",
    "2. **Result Caching** - Cache merged results to avoid repeated queries\n",
    "3. **Lazy Loading** - Only fetch results as needed\n",
    "4. **Max Pages Limit** - Prevent unbounded result sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b98be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package (if not already installed)\n",
    "# !pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class AzureRedisMultiIndexSearch:\n",
    "    \"\"\"\n",
    "    Optimized multi-index search with Azure Cache for Redis.\n",
    "    \n",
    "    Features:\n",
    "    - Caches merged search results in Azure Redis\n",
    "    - Lazy loading - only fetches what's needed\n",
    "    - Max pages limit to prevent unbounded queries\n",
    "    - TTL-based cache expiration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 redis_host: str,\n",
    "                 redis_password: str,\n",
    "                 redis_port: int = 6380,  # Azure Redis default SSL port\n",
    "                 redis_ssl: bool = True,   # Azure Redis requires SSL\n",
    "                 cache_ttl: int = 300,     # 5 minutes\n",
    "                 max_pages: int = 50,\n",
    "                 page_size: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize Azure Redis connection.\n",
    "        \n",
    "        Args:\n",
    "            redis_host: Azure Redis hostname (e.g., 'myredis.redis.cache.windows.net')\n",
    "            redis_password: Azure Redis access key (from Azure Portal)\n",
    "            redis_port: Port (6380 for SSL, 6379 for non-SSL)\n",
    "            redis_ssl: Use SSL (required for Azure Redis)\n",
    "            cache_ttl: Cache time-to-live in seconds\n",
    "            max_pages: Maximum number of pages to return\n",
    "            page_size: Number of results per page\n",
    "        \"\"\"\n",
    "        # Connect to Azure Redis\n",
    "        self.redis_client = redis.Redis(\n",
    "            host=redis_host,\n",
    "            port=redis_port,\n",
    "            password=redis_password,\n",
    "            ssl=redis_ssl,\n",
    "            ssl_cert_reqs=None,  # Azure manages certificates\n",
    "            decode_responses=True  # Return strings instead of bytes\n",
    "        )\n",
    "        \n",
    "        self.cache_ttl = cache_ttl\n",
    "        self.MAX_PAGES = max_pages\n",
    "        self.PAGE_SIZE = page_size\n",
    "        self.MAX_RESULTS = max_pages * page_size\n",
    "        \n",
    "        # Test connection\n",
    "        try:\n",
    "            self.redis_client.ping()\n",
    "            print(\"✅ Connected to Azure Redis Cache\")\n",
    "        except redis.ConnectionError as e:\n",
    "            print(f\"❌ Failed to connect to Azure Redis: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _generate_cache_key(self, search_text: str, filters: Optional[str] = None) -> str:\n",
    "        \"\"\"Generate unique cache key for search parameters.\"\"\"\n",
    "        key_data = f\"{search_text}:{filters or ''}\"\n",
    "        hash_key = hashlib.md5(key_data.encode()).hexdigest()\n",
    "        return f\"multi_index_search:{hash_key}\"\n",
    "    \n",
    "    def _get_cached_results(self, cache_key: str) -> Optional[List[Dict]]:\n",
    "        \"\"\"Retrieve cached results from Azure Redis.\"\"\"\n",
    "        try:\n",
    "            cached_data = self.redis_client.get(cache_key)\n",
    "            if cached_data:\n",
    "                print(f\"✅ Cache HIT - Retrieved from Azure Redis\")\n",
    "                return json.loads(cached_data)\n",
    "            else:\n",
    "                print(f\"❌ Cache MISS - Will query Azure AI Search\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Redis error: {e} - Proceeding without cache\")\n",
    "            return None\n",
    "    \n",
    "    def _cache_results(self, cache_key: str, results: List[Dict]):\n",
    "        \"\"\"Store results in Azure Redis with TTL.\"\"\"\n",
    "        try:\n",
    "            self.redis_client.setex(\n",
    "                cache_key,\n",
    "                self.cache_ttl,\n",
    "                json.dumps(results)\n",
    "            )\n",
    "            print(f\"💾 Cached {len(results)} results in Azure Redis (TTL: {self.cache_ttl}s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to cache results: {e}\")\n",
    "    \n",
    "    def search(self, \n",
    "               search_text: str, \n",
    "               page: int = 1,\n",
    "               filter_expr: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute multi-index search with caching and pagination.\n",
    "        \n",
    "        Args:\n",
    "            search_text: Search query\n",
    "            page: Page number (1-indexed)\n",
    "            filter_expr: Optional OData filter expression\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results, pagination info, and metadata\n",
    "        \"\"\"\n",
    "        # Validate page number\n",
    "        if page < 1:\n",
    "            raise ValueError(\"Page must be >= 1\")\n",
    "        if page > self.MAX_PAGES:\n",
    "            raise ValueError(f\"Page {page} exceeds maximum of {self.MAX_PAGES}\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Try to get from cache\n",
    "        cache_key = self._generate_cache_key(search_text, filter_expr)\n",
    "        merged_results = self._get_cached_results(cache_key)\n",
    "        \n",
    "        if merged_results is None:\n",
    "            # Cache miss - query Azure AI Search\n",
    "            print(f\"🔍 Querying {len(indexes)} indexes...\")\n",
    "            \n",
    "            # Calculate how many results we need per index\n",
    "            # We want enough to fill MAX_PAGES, distributed across indexes\n",
    "            per_index_limit = min(200, self.MAX_RESULTS // len(indexes) + 10)\n",
    "            \n",
    "            # Query all indexes\n",
    "            all_results = query_all_indexes(\n",
    "                search_text, \n",
    "                top=per_index_limit,\n",
    "                filter_expr=filter_expr\n",
    "            )\n",
    "            \n",
    "            if not all_results:\n",
    "                return {\n",
    "                    \"results\": [],\n",
    "                    \"page\": page,\n",
    "                    \"page_size\": self.PAGE_SIZE,\n",
    "                    \"total_results\": 0,\n",
    "                    \"total_pages\": 0,\n",
    "                    \"query_time_ms\": (datetime.now() - start_time).total_seconds() * 1000,\n",
    "                    \"cache_hit\": False\n",
    "                }\n",
    "            \n",
    "            # Normalize scores\n",
    "            all_results = normalize_scores(all_results)\n",
    "            \n",
    "            # Group by index for RRF\n",
    "            results_by_index = {\n",
    "                idx: [doc for doc in all_results if doc[\"__index\"] == idx] \n",
    "                for idx in indexes\n",
    "            }\n",
    "            \n",
    "            # Apply RRF merge\n",
    "            merged_results = rrf_merge(results_by_index)\n",
    "            \n",
    "            # Limit to max results\n",
    "            merged_results = merged_results[:self.MAX_RESULTS]\n",
    "            \n",
    "            # Cache the results\n",
    "            self._cache_results(cache_key, merged_results)\n",
    "            cache_hit = False\n",
    "        else:\n",
    "            cache_hit = True\n",
    "        \n",
    "        # Calculate pagination\n",
    "        total_results = len(merged_results)\n",
    "        total_pages = (total_results + self.PAGE_SIZE - 1) // self.PAGE_SIZE\n",
    "        \n",
    "        # Extract requested page\n",
    "        start_idx = (page - 1) * self.PAGE_SIZE\n",
    "        end_idx = start_idx + self.PAGE_SIZE\n",
    "        page_results = merged_results[start_idx:end_idx]\n",
    "        \n",
    "        query_time_ms = (datetime.now() - start_time).total_seconds() * 1000\n",
    "        \n",
    "        return {\n",
    "            \"results\": page_results,\n",
    "            \"page\": page,\n",
    "            \"page_size\": self.PAGE_SIZE,\n",
    "            \"total_results\": total_results,\n",
    "            \"total_pages\": total_pages,\n",
    "            \"has_next\": page < total_pages,\n",
    "            \"has_previous\": page > 1,\n",
    "            \"query_time_ms\": round(query_time_ms, 2),\n",
    "            \"cache_hit\": cache_hit,\n",
    "            \"indexes_searched\": indexes\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self, search_text: Optional[str] = None):\n",
    "        \"\"\"Clear cache for specific search or all searches.\"\"\"\n",
    "        if search_text:\n",
    "            cache_key = self._generate_cache_key(search_text)\n",
    "            self.redis_client.delete(cache_key)\n",
    "            print(f\"🗑️ Cleared cache for: {search_text}\")\n",
    "        else:\n",
    "            # Clear all multi-index search caches\n",
    "            pattern = \"multi_index_search:*\"\n",
    "            keys = self.redis_client.keys(pattern)\n",
    "            if keys:\n",
    "                self.redis_client.delete(*keys)\n",
    "                print(f\"🗑️ Cleared {len(keys)} cached searches\")\n",
    "            else:\n",
    "                print(\"ℹ️ No cached searches to clear\")\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict:\n",
    "        \"\"\"Get Redis cache statistics.\"\"\"\n",
    "        try:\n",
    "            info = self.redis_client.info('stats')\n",
    "            return {\n",
    "                \"total_connections\": info.get('total_connections_received', 0),\n",
    "                \"total_commands\": info.get('total_commands_processed', 0),\n",
    "                \"keyspace_hits\": info.get('keyspace_hits', 0),\n",
    "                \"keyspace_misses\": info.get('keyspace_misses', 0),\n",
    "                \"hit_rate\": round(\n",
    "                    info.get('keyspace_hits', 0) / \n",
    "                    max(info.get('keyspace_hits', 0) + info.get('keyspace_misses', 0), 1) * 100, \n",
    "                    2\n",
    "                )\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74372a",
   "metadata": {},
   "source": [
    "## Configure Azure Redis Connection\n",
    "\n",
    "Get your Azure Redis credentials from Azure Portal:\n",
    "1. Go to Azure Portal → Azure Cache for Redis\n",
    "2. Click on your Redis instance\n",
    "3. Go to \"Access keys\" blade\n",
    "4. Copy the \"Primary connection string\" or use host + key separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0a691ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redis Host: testredisforaisearch.redis.cache.windows.net\n",
      "Redis Port: 6380\n",
      "Redis SSL: True\n",
      "Redis Password: ✅ Set\n"
     ]
    }
   ],
   "source": [
    "# Azure Redis Configuration\n",
    "# Add these to your .env file:\n",
    "# REDIS_HOST=your-redis-name.redis.cache.windows.net\n",
    "# REDIS_PASSWORD=your-redis-access-key\n",
    "# REDIS_PORT=6380\n",
    "# REDIS_SSL=true\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Load from environment variables (no hardcoded values)\n",
    "redis_host = os.getenv(\"REDIS_HOST\")\n",
    "redis_password = os.getenv(\"REDIS_PASSWORD\")\n",
    "redis_port = int(os.getenv(\"REDIS_PORT\", \"6380\"))  # Azure Redis SSL port\n",
    "redis_ssl = os.getenv(\"REDIS_SSL\", \"true\").lower() == \"true\"\n",
    "\n",
    "print(f\"Redis Host: {redis_host}\")\n",
    "print(f\"Redis Port: {redis_port}\")\n",
    "print(f\"Redis SSL: {redis_ssl}\")\n",
    "print(f\"Redis Password: {'✅ Set' if redis_password else '❌ Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738ca51",
   "metadata": {},
   "source": [
    "## Usage Example: Multi-Index Search with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "169ca095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Azure Redis Cache\n",
      "\n",
      "============================================================\n",
      "FIRST SEARCH (Cache Miss)\n",
      "============================================================\n",
      "❌ Cache MISS - Will query Azure AI Search\n",
      "🔍 Querying 3 indexes...\n",
      "❌ Error: \n",
      "\n",
      "✅ Solution works without Redis - continue with cell 16!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimized search service\n",
    "# Check if Redis credentials are configured\n",
    "if not redis_host or not redis_password:\n",
    "    print(\"⚠️  Redis credentials not configured in .env file\")\n",
    "    print(\"ℹ️  Skipping Redis cache demo. To enable caching:\")\n",
    "    print(\"   1. Add REDIS_HOST to your .env file\")\n",
    "    print(\"   2. Add REDIS_PASSWORD to your .env file\")\n",
    "    print(\"   3. Ensure your IP is whitelisted in Azure Redis firewall\")\n",
    "    print(\"\\n✅ The solution works without Redis - it just won't have caching.\")\n",
    "    print(\"   Continue with cell 16 for the balanced pagination demo!\")\n",
    "else:\n",
    "    try:\n",
    "        search_service = AzureRedisMultiIndexSearch(\n",
    "            redis_host=redis_host,\n",
    "            redis_password=redis_password,\n",
    "            redis_port=redis_port,\n",
    "            redis_ssl=redis_ssl,\n",
    "            cache_ttl=300,      # 5 minutes\n",
    "            max_pages=50,       # Max 50 pages\n",
    "            page_size=10        # 10 results per page\n",
    "        )\n",
    "        \n",
    "        # First search - will query Azure AI Search and cache results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FIRST SEARCH (Cache Miss)\")\n",
    "        print(\"=\"*60)\n",
    "        result1 = search_service.search(\"cloud security\", page=1)\n",
    "        \n",
    "        print(f\"\\n📊 Results:\")\n",
    "        print(f\"   Page: {result1['page']} of {result1['total_pages']}\")\n",
    "        print(f\"   Total results: {result1['total_results']}\")\n",
    "        print(f\"   Query time: {result1['query_time_ms']} ms\")\n",
    "        print(f\"   Cache hit: {result1['cache_hit']}\")\n",
    "        print(f\"\\n📄 Top results:\")\n",
    "        for i, doc in enumerate(result1['results'][:3], 1):\n",
    "            print(f\"   {i}. [{doc['__index']}] {doc['title']}\")\n",
    "            print(f\"      RRF Score: {doc['rrf_score']:.4f}\")\n",
    "        \n",
    "        # Second search - same query, will use cache\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SECOND SEARCH - Same Query (Cache Hit)\")\n",
    "        print(\"=\"*60)\n",
    "        result2 = search_service.search(\"cloud security\", page=1)\n",
    "        \n",
    "        print(f\"\\n📊 Results:\")\n",
    "        print(f\"   Page: {result2['page']} of {result2['total_pages']}\")\n",
    "        print(f\"   Query time: {result2['query_time_ms']} ms ⚡\")\n",
    "        print(f\"   Cache hit: {result2['cache_hit']}\")\n",
    "        print(f\"   Speed improvement: {round(result1['query_time_ms'] / result2['query_time_ms'], 1)}x faster\")\n",
    "        \n",
    "        # Page navigation - will use same cached results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PAGE NAVIGATION (Cache Hit)\")\n",
    "        print(\"=\"*60)\n",
    "        result3 = search_service.search(\"cloud security\", page=2)\n",
    "        \n",
    "        print(f\"\\n📊 Results:\")\n",
    "        print(f\"   Page: {result3['page']} of {result3['total_pages']}\")\n",
    "        print(f\"   Query time: {result3['query_time_ms']} ms ⚡\")\n",
    "        print(f\"   Cache hit: {result3['cache_hit']}\")\n",
    "        print(f\"\\n📄 Page 2 results:\")\n",
    "        for i, doc in enumerate(result3['results'][:3], 1):\n",
    "            print(f\"   {i}. [{doc['__index']}] {doc['title']}\")\n",
    "            print(f\"      RRF Score: {doc['rrf_score']:.4f}\")\n",
    "        \n",
    "        # Get cache statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"REDIS CACHE STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        stats = search_service.get_cache_stats()\n",
    "        print(f\"   Cache hit rate: {stats.get('hit_rate', 'N/A')}%\")\n",
    "        print(f\"   Total commands: {stats.get('total_commands', 'N/A')}\")\n",
    "        print(f\"   Keyspace hits: {stats.get('keyspace_hits', 'N/A')}\")\n",
    "        print(f\"   Keyspace misses: {stats.get('keyspace_misses', 'N/A')}\")\n",
    "        \n",
    "    except redis.ConnectionError as e:\n",
    "        print(f\"❌ Cannot connect to Azure Redis: {e}\")\n",
    "        print(\"\\n📋 Troubleshooting Steps:\")\n",
    "        print(\"   1. Verify REDIS_HOST format: yourname.redis.cache.windows.net\")\n",
    "        print(\"   2. Get access key from Azure Portal → Redis → Access keys\")\n",
    "        print(\"   3. Whitelist your IP in Azure Portal → Redis → Firewall\")\n",
    "        print(\"   4. Ensure using port 6380 (SSL) not 6379\")\n",
    "        print(\"\\n✅ Solution works without Redis - continue with cell 16!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        print(\"\\n✅ Solution works without Redis - continue with cell 16!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65c83c",
   "metadata": {},
   "source": [
    "## Cache Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2f292ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cache Statistics:\n",
      "  total_connections: 63300\n",
      "  total_commands: 52093\n",
      "  keyspace_hits: 0\n",
      "  keyspace_misses: 1\n",
      "  hit_rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Clear cache for specific search\n",
    "# search_service.clear_cache(\"cloud security\")\n",
    "\n",
    "# Clear all cached searches\n",
    "# search_service.clear_cache()\n",
    "\n",
    "# Check current cache stats\n",
    "try:\n",
    "    stats = search_service.get_cache_stats()\n",
    "    print(\"Current Cache Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Redis not connected - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ad0f1",
   "metadata": {},
   "source": [
    "# Balanced Multi-Index Pagination\n",
    "\n",
    "To show results from **all indexes on each page**, we need a different strategy than pure RRF ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ae5984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BALANCED PAGINATION - Round Robin (Equal representation)\n",
      "======================================================================\n",
      "Each page tries to show results from ALL indexes\n",
      "\n",
      "📄 Page 1 of 5 (showing 6 results)\n",
      "Index distribution: {'finance': 2, 'hr': 2, 'engineering': 2}\n",
      "\n",
      "   1. [finance     ] Cloud Data Encryption                         | 2025-09-22T15:45:00Z\n",
      "   2. [hr          ] HR Data Protection                            | 2025-09-20T08:15:00Z\n",
      "   3. [engineering ] Cloud Security Architecture                   | 2025-09-30T11:00:00Z\n",
      "   4. [finance     ] Cybersecurity Incident Response               | 2025-09-24T13:30:00Z\n",
      "   5. [hr          ] Cloud Security Training                       | 2025-09-28T08:00:00Z\n",
      "   6. [engineering ] DevSecOps Pipeline                            | 2025-09-20T10:30:00Z\n",
      "\n",
      "📄 Page 2 of 5 (showing 6 results)\n",
      "Index distribution: {'finance': 2, 'hr': 2, 'engineering': 2}\n",
      "\n",
      "   1. [finance     ] Cloud Migration for Trading Systems           | 2025-09-27T11:15:00Z\n",
      "   2. [hr          ] Cloud Access Management                       | 2025-09-21T15:30:00Z\n",
      "   3. [engineering ] Zero Trust Network Architecture               | 2025-09-27T09:20:00Z\n",
      "   4. [finance     ] Fraud Detection with AI                       | 2025-09-19T14:15:00Z\n",
      "   5. [hr          ] Data Privacy Training                         | 2025-09-25T14:15:00Z\n",
      "   6. [engineering ] Cloud Database Encryption                     | 2025-09-21T14:00:00Z\n",
      "\n",
      "======================================================================\n",
      "WEIGHTED BALANCED PAGINATION (Proportional representation)\n",
      "======================================================================\n",
      "Indexes with more results get more space on each page\n",
      "\n",
      "📄 Page 1 of 5 (showing 6 results)\n",
      "Index distribution: {'finance': 2, 'hr': 2, 'engineering': 2}\n",
      "\n",
      "   1. [finance     ] Cloud Data Encryption                         | 2025-09-22T15:45:00Z\n",
      "   2. [hr          ] HR Data Protection                            | 2025-09-20T08:15:00Z\n",
      "   3. [engineering ] Cloud Security Architecture                   | 2025-09-30T11:00:00Z\n",
      "   4. [finance     ] Cybersecurity Incident Response               | 2025-09-24T13:30:00Z\n",
      "   5. [hr          ] Cloud Security Training                       | 2025-09-28T08:00:00Z\n",
      "   6. [engineering ] DevSecOps Pipeline                            | 2025-09-20T10:30:00Z\n"
     ]
    }
   ],
   "source": [
    "def get_page_balanced(results_by_index: Dict[str, List[Dict]], page_num=1, page_size=10):\n",
    "    \"\"\"\n",
    "    Balanced pagination - ensures each page has results from all indexes.\n",
    "    \n",
    "    Strategy: Round-robin interleaving\n",
    "    - Page 1: Top result from each index (finance, hr, eng), then next from each, etc.\n",
    "    - Page 2: Continue the pattern\n",
    "    \n",
    "    This ensures users see diverse results from all business areas on every page.\n",
    "    \n",
    "    Args:\n",
    "        results_by_index: Dictionary of index_name -> list of results (sorted by RRF)\n",
    "        page_num: Page number (1-indexed)\n",
    "        page_size: Results per page\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with balanced page results and metadata\n",
    "    \"\"\"\n",
    "    # Create iterators for each index (already sorted by RRF within each index)\n",
    "    index_iterators = {idx: iter(docs) for idx, docs in results_by_index.items()}\n",
    "    index_names = list(results_by_index.keys())\n",
    "    \n",
    "    # Interleave results using round-robin\n",
    "    all_results = []\n",
    "    exhausted = set()\n",
    "    \n",
    "    while len(exhausted) < len(index_names):\n",
    "        for idx_name in index_names:\n",
    "            if idx_name not in exhausted:\n",
    "                try:\n",
    "                    doc = next(index_iterators[idx_name])\n",
    "                    all_results.append(doc)\n",
    "                except StopIteration:\n",
    "                    exhausted.add(idx_name)\n",
    "    \n",
    "    # Now paginate the interleaved results\n",
    "    total_results = len(all_results)\n",
    "    total_pages = (total_results + page_size - 1) // page_size\n",
    "    \n",
    "    start = (page_num - 1) * page_size\n",
    "    end = start + page_size\n",
    "    page_results = all_results[start:end]\n",
    "    \n",
    "    # Calculate index distribution on this page\n",
    "    index_counts = {}\n",
    "    for doc in page_results:\n",
    "        idx = doc['__index']\n",
    "        index_counts[idx] = index_counts.get(idx, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        \"results\": page_results,\n",
    "        \"page\": page_num,\n",
    "        \"page_size\": page_size,\n",
    "        \"total_results\": total_results,\n",
    "        \"total_pages\": total_pages,\n",
    "        \"has_next\": page_num < total_pages,\n",
    "        \"has_previous\": page_num > 1,\n",
    "        \"index_distribution\": index_counts  # Shows how many from each index\n",
    "    }\n",
    "\n",
    "\n",
    "def get_page_balanced_weighted(results_by_index: Dict[str, List[Dict]], page_num=1, page_size=10):\n",
    "    \"\"\"\n",
    "    Weighted balanced pagination - proportional representation by result count.\n",
    "    \n",
    "    If finance has 8 results, hr has 4, engineering has 2:\n",
    "    - Page 1 might have: 4 finance, 2 hr, 1 engineering (roughly proportional)\n",
    "    \n",
    "    Better than pure round-robin when indexes have very different result counts.\n",
    "    \"\"\"\n",
    "    # Calculate total and proportions\n",
    "    total_count = sum(len(docs) for docs in results_by_index.values())\n",
    "    if total_count == 0:\n",
    "        return {\n",
    "            \"results\": [],\n",
    "            \"page\": page_num,\n",
    "            \"page_size\": page_size,\n",
    "            \"total_results\": 0,\n",
    "            \"total_pages\": 0,\n",
    "            \"has_next\": False,\n",
    "            \"has_previous\": False,\n",
    "            \"index_distribution\": {}\n",
    "        }\n",
    "    \n",
    "    # Calculate how many results each index should contribute per page\n",
    "    proportions = {\n",
    "        idx: len(docs) / total_count \n",
    "        for idx, docs in results_by_index.items()\n",
    "    }\n",
    "    \n",
    "    # Build interleaved list with weighted distribution\n",
    "    all_results = []\n",
    "    index_positions = {idx: 0 for idx in results_by_index.keys()}\n",
    "    \n",
    "    # Interleave based on proportions\n",
    "    while any(pos < len(results_by_index[idx]) for idx, pos in index_positions.items()):\n",
    "        for idx in sorted(results_by_index.keys(), key=lambda x: -proportions[x]):\n",
    "            pos = index_positions[idx]\n",
    "            if pos < len(results_by_index[idx]):\n",
    "                all_results.append(results_by_index[idx][pos])\n",
    "                index_positions[idx] += 1\n",
    "    \n",
    "    # Paginate\n",
    "    total_results = len(all_results)\n",
    "    total_pages = (total_results + page_size - 1) // page_size\n",
    "    \n",
    "    start = (page_num - 1) * page_size\n",
    "    end = start + page_size\n",
    "    page_results = all_results[start:end]\n",
    "    \n",
    "    # Calculate distribution\n",
    "    index_counts = {}\n",
    "    for doc in page_results:\n",
    "        idx = doc['__index']\n",
    "        index_counts[idx] = index_counts.get(idx, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        \"results\": page_results,\n",
    "        \"page\": page_num,\n",
    "        \"page_size\": page_size,\n",
    "        \"total_results\": total_results,\n",
    "        \"total_pages\": total_pages,\n",
    "        \"has_next\": page_num < total_pages,\n",
    "        \"has_previous\": page_num > 1,\n",
    "        \"index_distribution\": index_counts\n",
    "    }\n",
    "\n",
    "\n",
    "# Demo: Balanced Pagination\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BALANCED PAGINATION - Round Robin (Equal representation)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Each page tries to show results from ALL indexes\\n\")\n",
    "\n",
    "# Get results grouped by index (sorted by RRF within each)\n",
    "search_text = \"cloud security\"\n",
    "all_results = query_all_indexes(search_text, top=10)\n",
    "all_results = normalize_scores(all_results)\n",
    "results_by_index = {idx: [doc for doc in all_results if doc[\"__index\"] == idx] for idx in indexes}\n",
    "\n",
    "# Sort each index's results by RRF\n",
    "for idx in results_by_index:\n",
    "    docs = results_by_index[idx]\n",
    "    docs_sorted = sorted(docs, key=lambda x: x.get(\"norm_score\", 0), reverse=True)\n",
    "    for i, doc in enumerate(docs_sorted):\n",
    "        doc['rrf_score'] = 1 / (i + 1 + 60)  # Simple RRF for demo\n",
    "    results_by_index[idx] = docs_sorted\n",
    "\n",
    "# Page 1\n",
    "page1 = get_page_balanced(results_by_index, page_num=1, page_size=6)\n",
    "print(f\"📄 Page {page1['page']} of {page1['total_pages']} (showing {len(page1['results'])} results)\")\n",
    "print(f\"Index distribution: {page1['index_distribution']}\")\n",
    "print()\n",
    "for i, doc in enumerate(page1['results'], 1):\n",
    "    print(f\"   {i}. [{doc['__index']:12}] {doc['title'][:45]:45} | {doc['timestamp']}\")\n",
    "\n",
    "# Page 2\n",
    "if page1['has_next']:\n",
    "    page2 = get_page_balanced(results_by_index, page_num=2, page_size=6)\n",
    "    print(f\"\\n📄 Page {page2['page']} of {page2['total_pages']} (showing {len(page2['results'])} results)\")\n",
    "    print(f\"Index distribution: {page2['index_distribution']}\")\n",
    "    print()\n",
    "    for i, doc in enumerate(page2['results'], 1):\n",
    "        print(f\"   {i}. [{doc['__index']:12}] {doc['title'][:45]:45} | {doc['timestamp']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEIGHTED BALANCED PAGINATION (Proportional representation)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Indexes with more results get more space on each page\\n\")\n",
    "\n",
    "# Weighted version\n",
    "page1_weighted = get_page_balanced_weighted(results_by_index, page_num=1, page_size=6)\n",
    "print(f\"📄 Page {page1_weighted['page']} of {page1_weighted['total_pages']} (showing {len(page1_weighted['results'])} results)\")\n",
    "print(f\"Index distribution: {page1_weighted['index_distribution']}\")\n",
    "print()\n",
    "for i, doc in enumerate(page1_weighted['results'], 1):\n",
    "    print(f\"   {i}. [{doc['__index']:12}] {doc['title'][:45]:45} | {doc['timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9f839",
   "metadata": {},
   "source": [
    "# Expanded Dataset Demo\n",
    "\n",
    "Now let's reload the expanded datasets (20 documents per index = 60 total) and demonstrate pagination with multiple pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate indexes with expanded data\n",
    "print(\"🔄 Recreating indexes with expanded datasets...\")\n",
    "for idx in indexes:\n",
    "    create_index(idx)\n",
    "\n",
    "print(\"\\n📤 Uploading expanded documents...\")\n",
    "for idx, file in csv_files.items():\n",
    "    upload_documents(idx, file)\n",
    "\n",
    "print(\"\\n✅ Indexes recreated with 20 documents each (60 total)\")\n",
    "\n",
    "# Search and show dataset statistics\n",
    "search_text = \"cloud security\"\n",
    "all_results = query_all_indexes(search_text, top=50)  # Fetch more results\n",
    "all_results = normalize_scores(all_results)\n",
    "\n",
    "print(f\"\\n📊 Dataset Statistics:\")\n",
    "print(f\"   Total results for '{search_text}': {len(all_results)}\")\n",
    "for idx in indexes:\n",
    "    count = len([doc for doc in all_results if doc[\"__index\"] == idx])\n",
    "    print(f\"   {idx:12}: {count} documents\")\n",
    "\n",
    "# Prepare for balanced pagination\n",
    "results_by_index = {idx: [doc for doc in all_results if doc[\"__index\"] == idx] for idx in indexes}\n",
    "\n",
    "# Sort each index's results by score\n",
    "for idx in results_by_index:\n",
    "    docs = results_by_index[idx]\n",
    "    docs_sorted = sorted(docs, key=lambda x: x.get(\"norm_score\", 0), reverse=True)\n",
    "    for i, doc in enumerate(docs_sorted):\n",
    "        doc['rrf_score'] = 1 / (i + 1 + 60)\n",
    "    results_by_index[idx] = docs_sorted\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BALANCED PAGINATION - Multiple Pages Demo\")\n",
    "print(\"=\"*80)\n",
    "print(\"Each page shows results from ALL indexes (round-robin interleaving)\\n\")\n",
    "\n",
    "# Show first 5 pages with 10 results each\n",
    "for page_num in range(1, 6):\n",
    "    page = get_page_balanced(results_by_index, page_num=page_num, page_size=10)\n",
    "    \n",
    "    if not page['results']:\n",
    "        break\n",
    "        \n",
    "    print(f\"\\n📄 Page {page['page']} of {page['total_pages']}\")\n",
    "    print(f\"   Index distribution: {page['index_distribution']}\")\n",
    "    print(f\"   Showing {len(page['results'])} results:\")\n",
    "    \n",
    "    for i, doc in enumerate(page['results'], 1):\n",
    "        # Truncate title for display\n",
    "        title_short = doc['title'][:50].ljust(50)\n",
    "        print(f\"      {i:2}. [{doc['__index']:12}] {title_short} | {doc['timestamp'][:10]}\")\n",
    "    \n",
    "    if not page['has_next']:\n",
    "        print(\"\\n   ℹ️  No more pages\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✅ Successfully demonstrated pagination across {page['total_pages']} pages\")\n",
    "print(f\"   Each page maintained balanced representation from all {len(indexes)} indexes\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubblingvenv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
